{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "\n",
    "class UserItemRatingDataset(Dataset):\n",
    "    \"\"\"Wrapper, convert <user, item, rating> Tensor into Pytorch Dataset\"\"\"\n",
    "    def __init__(self, user_tensor, item_tensor, target_tensor):\n",
    "        \"\"\"\n",
    "        args:\n",
    "\n",
    "            target_tensor: torch.Tensor, the corresponding rating for <user, item> pair\n",
    "        \"\"\"\n",
    "        self.user_tensor = user_tensor\n",
    "        self.item_tensor = item_tensor\n",
    "        self.target_tensor = target_tensor\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.user_tensor[index], self.item_tensor[index], self.target_tensor[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.user_tensor.size(0)\n",
    "\n",
    "\n",
    "class SampleGenerator(object):\n",
    "    \"\"\"Construct dataset for NCF\"\"\"\n",
    "\n",
    "    def __init__(self, ratings):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            ratings: pd.DataFrame, which contains 4 columns = ['userId', 'itemId', 'rating', 'timestamp']\n",
    "        \"\"\"\n",
    "        assert 'userId' in ratings.columns\n",
    "        assert 'itemId' in ratings.columns\n",
    "        assert 'rating' in ratings.columns\n",
    "\n",
    "        self.ratings = ratings\n",
    "        # explicit feedback using _normalize and implicit using _binarize\n",
    "        # self.preprocess_ratings = self._normalize(ratings)\n",
    "        self.preprocess_ratings = self._binarize(ratings)\n",
    "        self.user_pool = set(self.ratings['userId'].unique())\n",
    "        self.item_pool = set(self.ratings['itemId'].unique())\n",
    "        # create negative item samples for NCF learning\n",
    "        self.negatives = self._sample_negative(ratings)\n",
    "        self.train_ratings, self.test_ratings = self._split_loo(self.preprocess_ratings)\n",
    "        \n",
    "        file_train = \"train_ratings\"\n",
    "        train_rating_binary = self.train_ratings[['userId', 'itemId']]\n",
    "        # print(train_rating_binary)\n",
    "        train_rating_binary.to_csv(file_train, header=None, index=None)\n",
    "\n",
    "    def _normalize(self, ratings):\n",
    "        \"\"\"normalize into [0, 1] from [0, max_rating], explicit feedback\"\"\"\n",
    "        ratings = deepcopy(ratings)\n",
    "        max_rating = ratings.rating.max()\n",
    "        ratings['rating'] = ratings.rating * 1.0 / max_rating\n",
    "        return ratings\n",
    "    \n",
    "    def _binarize(self, ratings):\n",
    "        \"\"\"binarize into 0 or 1, implicit feedback\"\"\"\n",
    "        ratings = deepcopy(ratings)\n",
    "        ratings['rating'][ratings['rating'] > 0] = 1  #和下面的LOO相关\n",
    "        return ratings\n",
    "\n",
    "    def _split_loo(self, ratings):\n",
    "        \"\"\"leave one out train/test split \"\"\"\n",
    "        ratings['rank_latest'] = ratings.groupby(['userId'])['timestamp'].rank(method='first', ascending=False)\n",
    "        test = ratings[ratings['rank_latest'] == 1] # explicit时 test loo 选择评分最高的那一个gt : 当负采样采用的是1为阈值时，最好在这里随机取，如果采用4为阈值，LOO的评分越高越好\n",
    "        train = ratings[ratings['rank_latest'] > 1]\n",
    "        assert train['userId'].nunique() == test['userId'].nunique()\n",
    "        return train[['userId', 'itemId', 'rating']], test[['userId', 'itemId', 'rating']]\n",
    "\n",
    "    def _sample_negative(self, ratings):\n",
    "        \"\"\"return all negative items & 100 sampled negative items\"\"\"\n",
    "        interact_status = ratings.groupby('userId')['itemId'].apply(set).reset_index().rename(\n",
    "            columns={'itemId': 'interacted_items'})\n",
    "        interact_status['negative_items'] = interact_status['interacted_items'].apply(lambda x: self.item_pool - x)\n",
    "        interact_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, 99))\n",
    "        return interact_status[['userId', 'negative_items', 'negative_samples']]\n",
    "    \n",
    "    \n",
    "\n",
    "    def instance_a_train_loader(self, num_negatives, batch_size):\n",
    "        \"\"\"instance train loader for one training epoch\"\"\"\n",
    "        users, items, ratings = [], [], []\n",
    "        train_ratings = pd.merge(self.train_ratings, self.negatives[['userId', 'negative_items']], on='userId')\n",
    "        train_ratings['negatives'] = train_ratings['negative_items'].apply(lambda x: random.sample(x, num_negatives))\n",
    "        for row in train_ratings.itertuples():\n",
    "            users.append(int(row.userId))\n",
    "            items.append(int(row.itemId))\n",
    "            ratings.append(float(row.rating))\n",
    "            for i in range(num_negatives):\n",
    "                users.append(int(row.userId))\n",
    "                items.append(int(row.negatives[i]))\n",
    "                ratings.append(float(0))  # negative samples get 0 rating\n",
    "        dataset = UserItemRatingDataset(user_tensor=torch.LongTensor(users),\n",
    "                                        item_tensor=torch.LongTensor(items),\n",
    "                                        target_tensor=torch.FloatTensor(ratings))\n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    @property\n",
    "    def evaluate_data(self):\n",
    "        \"\"\"create evaluate data\"\"\"\n",
    "        file_name = 'test_negative_ratings'\n",
    "        test_ratings = pd.merge(self.test_ratings, self.negatives[['userId', 'negative_samples']], on='userId')\n",
    "        test_users, test_items, negative_users, negative_items = [], [], [], []\n",
    "        with open(file_name, 'w') as f:\n",
    "            for row in test_ratings.itertuples():\n",
    "                test_users.append(int(row.userId))\n",
    "                test_items.append(int(row.itemId))\n",
    "                t_rating = (int(row.userId), int(row.itemId))\n",
    "                f.write(str(t_rating))\n",
    "                for i in range(len(row.negative_samples)):\n",
    "                    negative_users.append(int(row.userId))\n",
    "                    negative_items.append(int(row.negative_samples[i]))\n",
    "                    f.write('\\t')\n",
    "                    f.write(str(row.negative_samples[i]))\n",
    "                f.write('\\n')\n",
    "        return [torch.LongTensor(test_users), torch.LongTensor(test_items), torch.LongTensor(negative_users),\n",
    "                torch.LongTensor(negative_items)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        userId  itemId  rating  timestamp\n",
      "0            0       0       5  978300760\n",
      "1            0       1       3  978302109\n",
      "2            0       2       3  978301968\n",
      "3            0       3       4  978300275\n",
      "4            0       4       5  978824291\n",
      "...        ...     ...     ...        ...\n",
      "999606    6039    2893       5  956715569\n",
      "999607    6039    2937       1  956716438\n",
      "999608    6039    3018       5  956704305\n",
      "999609    6039    3022       3  960971992\n",
      "999610    6039    3344       5  956704191\n",
      "\n",
      "[999611 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ml1m_dir = 'ratings.csv'\n",
    "ml1m_rating = pd.read_csv(ml1m_dir)\n",
    "# ml1m_rating = pd.read_csv(ml1m_dir, sep='::', header=None, names=['uid', 'mid', 'rating', 'timestamp'],  engine='python')\n",
    "ml1m_rating.columns = ['userId', 'itemId', 'rating', 'timestamp']\n",
    "print(ml1m_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of userId is [0, 6039]\n",
      "Range of itemId is [0, 3705]\n"
     ]
    }
   ],
   "source": [
    "user_id = ml1m_rating[['uid']].drop_duplicates().reindex()\n",
    "user_id['userId'] = np.arange(len(user_id))\n",
    "ml1m_rating = pd.merge(ml1m_rating, user_id, on=['uid'], how='left')  # merge 是很好的借助pandas进行index的方法\n",
    "item_id = ml1m_rating[['mid']].drop_duplicates()\n",
    "item_id['itemId'] = np.arange(len(item_id))\n",
    "ml1m_rating = pd.merge(ml1m_rating, item_id, on=['mid'], how='left')\n",
    "ml1m_rating = ml1m_rating[['userId', 'itemId', 'rating', 'timestamp']]\n",
    "print('Range of userId is [{}, {}]'.format(ml1m_rating.userId.min(), ml1m_rating.userId.max()))\n",
    "print('Range of itemId is [{}, {}]'.format(ml1m_rating.itemId.min(), ml1m_rating.itemId.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "        userId  itemId  rating\n",
      "0            0       0       1\n",
      "1            0       1       1\n",
      "2            0       2       1\n",
      "3            0       3       1\n",
      "4            0       4       1\n",
      "...        ...     ...     ...\n",
      "999606    6039    2893       1\n",
      "999607    6039    2937       1\n",
      "999608    6039    3018       1\n",
      "999609    6039    3022       1\n",
      "999610    6039    3344       1\n",
      "\n",
      "[993571 rows x 3 columns]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(min(ml1m_rating.groupby('itemId').agg('count').userId))\n",
    "sample_generator = SampleGenerator(ratings=ml1m_rating)\n",
    "evaluate_data = sample_generator.evaluate_data\n",
    "print(sample_generator.train_ratings)\n",
    "print(type(sample_generator.evaluate_data))\n",
    "# print('user:', min(ml1m_rating.groupby('userId').agg('count').userId))\n",
    "# print('item:', min(ml1m_rating.groupby('itemId').agg('count').itemId))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机将整个数据集划分为三部分，用于target, shadow, 和 vectorization, 满足MIA_rs的设定，需要保证三个数据集的item集合是一致的\n",
    "from tqdm import tqdm\n",
    "np.random.seed(0)\n",
    "train_df = pd.DataFrame([], columns=['uid', 'iid', 'rating'])\n",
    "test_df = pd.DataFrame([], columns=['uid', 'iid', 'rating'])\n",
    "users = pdr['uid'].unique()\n",
    "for i in tqdm(range(len(users))):\n",
    "    user_data = pdr[pdr['uid'] == i]\n",
    "    total = len(user_data)\n",
    "\n",
    "    n_train = int(total * 0.5)\n",
    "    train_idx = np.random.choice(total, n_train, replace=False)\n",
    "\n",
    "    n_test = total - n_train\n",
    "    test_idx = list(set(np.arange(total)) - set(train_idx))\n",
    "\n",
    "    user_train = user_data.iloc[np.sort(train_idx), :]\n",
    "    user_test = user_data.iloc[np.sort(test_idx), :]\n",
    "\n",
    "    train_df = train_df.append(user_train, ignore_index=True)\n",
    "    test_df = test_df.append(user_test, ignore_index=True)\n",
    "\n",
    "train_df['rating'] = np.float16(train_df['rating'])\n",
    "test_df['rating'] = np.float16(test_df['rating'])\n",
    "train_df.to_csv('squ0_train_shadow.csv', header=None, index=False)\n",
    "test_df.to_csv('squ0_test_shadow.csv', header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        userId  itemId  rating  timestamp\n",
      "0            0       0       5  978300760\n",
      "1            0       1       3  978302109\n",
      "2            0       2       3  978301968\n",
      "3            0       3       4  978300275\n",
      "4            0       4       5  978824291\n",
      "...        ...     ...     ...        ...\n",
      "999740    6037     213       2  956707005\n",
      "999741    6037     243       1  956715051\n",
      "999742    6037     273       3  956707604\n",
      "999743    6037     798       3  956706827\n",
      "999744    6037     183       5  956707547\n",
      "\n",
      "[502737 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "user_list = list(np.arange(6040))\n",
    "member_list = random.sample(user_list, 3020)\n",
    "member_dict={}\n",
    "att_list=[]\n",
    "for x in range(6040):\n",
    "    if x in member_list:\n",
    "        member_dict[x] = 1\n",
    "    else:\n",
    "        member_dict[x] = 0\n",
    "    att_list.append(member_dict[x])\n",
    "np.save('mia/my_file.npy', member_dict)\n",
    "df = {\"userId\": user_list, \"member\": att_list}\n",
    "df = pd.core.frame.DataFrame(df)\n",
    "ml1m_rating_pp = pd.merge(ml1m_rating, df, on=['userId'], how='left')\n",
    "\n",
    "member_pd = ml1m_rating_pp[ml1m_rating_pp.member==1][['userId', 'itemId', 'rating', 'timestamp']]\n",
    "non_member_pd = ml1m_rating_pp[ml1m_rating_pp.member==0][['userId', 'itemId', 'rating', 'timestamp']]\n",
    "\n",
    "print(member_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         userId  itemId  rating\n",
      "0             0       0       1\n",
      "1             0       1       1\n",
      "2             0       2       1\n",
      "3             0       3       1\n",
      "5             0       5       1\n",
      "...         ...     ...     ...\n",
      "1000204    6039     772       1\n",
      "1000205    6039    1106       1\n",
      "1000206    6039     365       1\n",
      "1000207    6039     152       1\n",
      "1000208    6039      26       1\n",
      "\n",
      "[994169 rows x 3 columns]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# sample_generator = SampleGenerator(ratings=ml1m_rating)\n",
    "sample_generator = SampleGenerator(ratings=ml1m_rating)\n",
    "evaluate_data = sample_generator.evaluate_data\n",
    "print(sample_generator_mem.train_ratings)\n",
    "print(type(sample_generator_mem.evaluate_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "file_name = 'mia/test_negative_ratings'\n",
    "with open(file_name, 'r') as f:\n",
    "    ll = f.readline()\n",
    "arr = ll.split('\\t')\n",
    "u = eval(arr[0])[0]\n",
    "\n",
    "print(eval(arr[0])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 18, 20, 42, 47, 48, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174}\n",
      "{0, 18, 20, 42, 47, 48, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train_data_a = pd.read_csv(\n",
    "\t\t\"mia/train_ratings\", \n",
    "\t\theader=None, names=['user', 'item'], \n",
    "\t\t dtype={0: np.int32, 1: np.int32})\n",
    "\n",
    "train_data_b = pd.read_csv(\n",
    "\t\t\"/home/xcl-python/NCF/Data/ml-1m.train.rating\", \n",
    "\t\tsep='\\t', header=None, names=['user', 'item'], \n",
    "\t\tusecols=[0, 1], dtype={0: np.int32, 1: np.int32})\n",
    "\n",
    "set_a = set(train_data_a[train_data_a.user==1]['item'].unique())\n",
    "set_b = set(train_data_b[train_data_b.user==1]['item'].unique())\n",
    "print(set_a)\n",
    "print(set_b)\n",
    "# sum = 0\n",
    "# sum_a = 0\n",
    "# sum_b = 0\n",
    "# for i in range(6040):\n",
    "#     set_a = set(train_data_a[train_data_a.user==i]['item'].unique())\n",
    "#     set_b = set(train_data_b[train_data_b.user==i]['item'].unique())\n",
    "#     sum += len(set_a) - len(set_b)\n",
    "#     sum_a += len(set_a)\n",
    "#     sum_b += len(set_b)\n",
    "# print(sum)\n",
    "# print(sum_a)\n",
    "# print(sum_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(\n",
    "\t\t\"/home/xcl-python/NCF/Data/ml-1m.train.rating\", \n",
    "\t\tsep='\\t', header=None, names=['user', 'item'], \n",
    "\t\tusecols=[0, 1], dtype={0: np.int32, 1: np.int32})\n",
    "set_b = set(train_data[train_data.user==0]['item'].unique())\n",
    "print(len(set_a - set_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "evaluate_data = sample_generator.evaluate_data\n",
    "print(type(evaluate_data[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d06ba2cbc6d386932ec799003af452b3f8f447f3c16d58a893deb8e8676a6535"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
